Information retrieval (IR) studies how to find relevant documents given a user query. In modern vector-based retrieval, textual data are encoded into dense vectors using embedding models; similarity search in vector space identifies candidate passages or documents. FAISS is an open-source library by Facebook AI Research for efficient similarity search over high-dimensional vectors. It supports multiple indexing strategies from exact search (IndexFlat) to large-scale ANN indices (IVF, HNSW, PQ).

A typical RAG pipeline combines retrieval and generation. First, a retriever identifies top-k relevant passages given an embedding of the query. Then a generator—typically a language model—is conditioned on the retrieved passages and query to produce an answer. This decoupling reduces the generator's reliance on memorization and helps ground the output in factual context.

Key IR considerations include chunking strategies (passages vs. documents), embedding model selection, similarity metric (cosine vs. inner product), index construction, and retrieval-time latency. Chunk size affects recall: too-large chunks may dilute relevance; too-small chunks may fragment meaning. Normalizing vectors and using inner-product indices is a common approach for cosine similarity in FAISS. For larger collections, approximate indices reduce memory and speed up search with minor loss in accuracy.

Evaluation for retrieval uses recall@k, MRR (mean reciprocal rank), and downstream metrics such as answer F1. Retrieval augmentation improves factuality and reduces hallucinations in generators, but it also introduces latency and requires careful prompt design to ensure generators use retrieved context appropriately.